{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPV3R9SqTSR5dMrPsUG4Rwq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##1.0 Environment Setup"],"metadata":{"id":"QjdjhoGV-K-4"}},{"cell_type":"code","source":["# Simple Fixed Version - Run this cell first\n","!apt-get update -qq > /dev/null\n","!apt-get install -y openjdk-11-jdk-headless -qq > /dev/null\n","!pip install pyspark==3.5.0 opencv-python-headless -q"],"metadata":{"id":"zO5foL0u-QdR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762950278438,"user_tz":-120,"elapsed":65016,"user":{"displayName":"Iwumansi Ebe","userId":"05698353981490023564"}},"outputId":"32026242-fa18-4381-ac45-9817ab09dd55"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.9/316.9 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","dataproc-spark-connect 0.8.3 requires pyspark[connect]~=3.5.1, but you have pyspark 3.5.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"markdown","source":["##2.0 Mount Drive"],"metadata":{"id":"R8ZNga-Qs8E4"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"nIyeIX3Cs-6B","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762961981107,"user_tz":-120,"elapsed":29299,"user":{"displayName":"Iwumansi Ebe","userId":"05698353981490023564"}},"outputId":"54fdce5e-2be6-4613-f12d-11651e864442"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["##3.0 Import and Configuration"],"metadata":{"id":"vM2ItjCY56gE"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","from pyspark.sql.functions import udf\n","from pyspark.sql.types import DoubleType\n","import cv2, os, shutil, numpy as np\n","from PIL import Image\n","import logging\n","from pathlib import Path\n","\n","# Configure logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","# Configuration\n","PATCHES_DIR = '/content/drive/MyDrive/Colab Notebooks/WSI-MIL-Pipeline/data/patches_normalized'\n","OUT_DIR = '/content/drive/MyDrive/Colab Notebooks/WSI-MIL-Pipeline/data/patches'\n","os.makedirs(OUT_DIR, exist_ok=True)"],"metadata":{"id":"tRVpJ8DR5_rP","executionInfo":{"status":"ok","timestamp":1762962003185,"user_tz":-120,"elapsed":1667,"user":{"displayName":"Iwumansi Ebe","userId":"05698353981490023564"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["##4.0 Tissue Fraction Function0"],"metadata":{"id":"o9KL8Gpc6BzT"}},{"cell_type":"code","source":["def tissue_fraction_improved(path):\n","    \"\"\"Calculate tissue fraction with comprehensive error handling\"\"\"\n","    try:\n","        if not os.path.exists(path):\n","            return 0.0\n","\n","        img = cv2.imread(path)\n","        if img is None or img.size == 0:\n","            return 0.0\n","\n","        # Convert to HSV and calculate tissue area\n","        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n","        # More robust tissue detection\n","        saturation = hsv[:,:,1]\n","        value = hsv[:,:,2]\n","\n","        # Combined criteria for tissue detection\n","        tissue_mask = (value < 240) & (saturation > 10)\n","        return float(np.mean(tissue_mask))\n","\n","    except Exception as e:\n","        logger.warning(f\"Failed to process {path}: {e}\")\n","        return 0.0"],"metadata":{"id":"1DSn0r8W6NEx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##5.0 Distributed File Operation Function"],"metadata":{"id":"KX7a2hpe6R5R"}},{"cell_type":"code","source":["def copy_files_distributed(row_iterator):\n","    \"\"\"Process files in distributed manner\"\"\"\n","    for row in row_iterator:\n","        try:\n","            source_path = row.path\n","            slide_name = row.slide\n","            dest_dir = Path(OUT_DIR) / slide_name\n","            dest_dir.mkdir(parents=True, exist_ok=True)\n","\n","            # Copy file (consider shutil.copy2 for metadata preservation)\n","            shutil.copy2(source_path, dest_dir)\n","\n","        except Exception as e:\n","            logger.error(f\"Failed to copy {source_path}: {e}\")"],"metadata":{"id":"BeWkfLkl6Ys1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##6.0 Main Orchestration Logic Funtion"],"metadata":{"id":"zWxyeZwt6cNF"}},{"cell_type":"code","source":["def main_improved():\n","    \"\"\"Improved main function with better error handling and performance\"\"\"\n","\n","    # Validate input directory\n","    if not os.path.exists(PATCHES_DIR):\n","        raise ValueError(f\"Input directory {PATCHES_DIR} does not exist\")\n","\n","    spark = SparkSession.builder \\\n","        .master('local[*]') \\\n","        .appName('TissueSegmentation') \\\n","        .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n","        .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n","        .getOrCreate()\n","\n","    try:\n","        # Build DataFrame more efficiently\n","        rows = []\n","        for slide in os.listdir(PATCHES_DIR):\n","            slide_dir = os.path.join(PATCHES_DIR, slide)\n","            if not os.path.isdir(slide_dir):\n","                continue\n","\n","            for fn in os.listdir(slide_dir):\n","                if fn.lower().endswith(('.png', '.jpg', '.jpeg', '.tiff')):\n","                    full_path = os.path.join(slide_dir, fn)\n","                    rows.append((slide, full_path))\n","\n","        if not rows:\n","            logger.warning(\"No patch files found!\")\n","            return\n","\n","        df = spark.createDataFrame(rows, ['slide', 'path'])\n","\n","        # Register UDF with better error handling\n","        tissue_udf = udf(tissue_fraction_improved, DoubleType())\n","\n","        # Filter and process\n","        filtered_df = df.filter(tissue_udf(df.path) > 0.05)\n","\n","        # Process in distributed manner\n","        filtered_df.foreachPartition(copy_files_distributed)\n","\n","        # Log statistics\n","        total_count = df.count()\n","        filtered_count = filtered_df.count()\n","        logger.info(f\"Processed {total_count} patches, kept {filtered_count} \"\n","                   f\"({filtered_count/total_count*100:.1f}%)\")\n","\n","    except Exception as e:\n","        logger.error(f\"Pipeline failed: {e}\")\n","        raise\n","\n","    finally:\n","        spark.stop()"],"metadata":{"id":"fVd6xrg_6j-Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##7.0 Execution Block"],"metadata":{"id":"5Y1-WZh36npq"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    main_improved()"],"metadata":{"id":"rgIr13ZT6rvU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8Izb9HoD5QFQ"},"execution_count":null,"outputs":[]}]}